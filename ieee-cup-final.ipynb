{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n\n!pip install git+https://github.com/huggingface/datasets.git\n!pip install git+https://github.com/huggingface/transformers.git\n!pip install jiwer\n!pip install torchaudio\n!pip install librosa\n\n# Monitor the training process\n# !pip install wandb","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-31T02:09:10.361835Z","iopub.execute_input":"2022-03-31T02:09:10.362258Z","iopub.status.idle":"2022-03-31T02:10:32.088412Z","shell.execute_reply.started":"2022-03-31T02:09:10.362168Z","shell.execute_reply":"2022-03-31T02:10:32.087418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%env LC_ALL=C.UTF-8\n%env LANG=C.UTF-8\n%env TRANSFORMERS_CACHE=/content/cache\n%env HF_DATASETS_CACHE=/content/cache\n%env CUDA_LAUNCH_BLOCKING=1","metadata":{"execution":{"iopub.status.busy":"2022-03-31T02:10:32.091674Z","iopub.execute_input":"2022-03-31T02:10:32.092407Z","iopub.status.idle":"2022-03-31T02:10:32.104664Z","shell.execute_reply.started":"2022-03-31T02:10:32.092367Z","shell.execute_reply":"2022-03-31T02:10:32.103973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom pathlib import Path\nfrom tqdm import tqdm\n\nimport torchaudio\nfrom sklearn.model_selection import train_test_split\n\nimport os\nimport sys","metadata":{"execution":{"iopub.status.busy":"2022-03-31T02:10:32.106083Z","iopub.execute_input":"2022-03-31T02:10:32.106509Z","iopub.status.idle":"2022-03-31T02:10:34.580889Z","shell.execute_reply.started":"2022-03-31T02:10:32.106472Z","shell.execute_reply":"2022-03-31T02:10:34.580063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Downloading Unseen augmented files:\n","metadata":{}},{"cell_type":"code","source":"!pip install --upgrade --no-cache-dir gdown\n!gdown --id 1-0PB4s_VS2IRkbsDaHDnIMJPaI0_mjRQ\n!unzip ./augmented_unseen.zip\n!rm augmented_unseen.zip","metadata":{"execution":{"iopub.status.busy":"2022-03-31T02:10:34.583382Z","iopub.execute_input":"2022-03-31T02:10:34.583617Z","iopub.status.idle":"2022-03-31T02:12:14.02345Z","shell.execute_reply.started":"2022-03-31T02:10:34.583583Z","shell.execute_reply":"2022-03-31T02:12:14.022473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!unzip ./augmented_unseen.zip\n!rm augmented_unseen.zip","metadata":{"execution":{"iopub.status.busy":"2022-03-31T02:12:14.028433Z","iopub.execute_input":"2022-03-31T02:12:14.028692Z","iopub.status.idle":"2022-03-31T02:12:15.348327Z","shell.execute_reply.started":"2022-03-31T02:12:14.02866Z","shell.execute_reply":"2022-03-31T02:12:15.347422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Reading Necessery Files","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\noriginal_labels = pd.read_csv(\"../input/spcup2022/spcup_2022_training_part1/spcup_2022_training_part1/labels.csv\")\noriginal_labels.iloc[:, 0] = \"../input/spcup2022/spcup_2022_training_part1/spcup_2022_training_part1/\" + original_labels.iloc[:, 0] \noriginal_labels.rename({'track': 'path', 'algorithm': 'label'}, axis=1, inplace=True)\noriginal_labels.reset_index(inplace=True, drop=True)\n\n# noisy_labels = pd.read_csv(\"/content/spcup_2022_training_part1/labels.csv\")\n# noisy_labels.iloc[:, 0] = noisy_labels.iloc[:, 0].str.split('.').str[0] + \"_noise.wav\"\n# noisy_labels.iloc[:, 0] = \"/content/noisyaugmentedfiles/\" + noisy_labels.iloc[:, 0]\n\n# reverberated_labels = pd.read_csv(\"/content/spcup_2022_training_part1/labels.csv\")\n# reverberated_labels.iloc[:, 0] = reverberated_labels.iloc[:, 0].str.split('.').str[0] + '_reverb.wav'\n# reverberated_labels.iloc[:, 0] = \"/content/reverberatedaugmentedfiles/\" + reverberated_labels.iloc[:, 0]\n\nunseen = pd.read_csv(\"../input/spcup2022/spcup_2022_unseen/spcup_2022_unseen/labels.csv\")\nunseen.iloc[:, 0] = \"../input/spcup2022/spcup_2022_unseen/spcup_2022_unseen/\" + unseen.iloc[:, 0]\n\n# unseen_aug = pd.read_csv(\"./output/labels.csv\")\n# unseen_aug.iloc[:, -1] = \"./output/\" + unseen_aug.iloc[:, -1]\n# unseen_aug = unseen_aug.iloc[:, [1, 3]]\n# unseen_aug.rename({'label': 'path', 'output_name':'label'}, axis=1, inplace=True)\n# unseen_aug.iloc[:,0], unseen_aug.iloc[:,1] = unseen_aug.iloc[:,1], unseen_aug.iloc[:,0]\n\nunseen.rename({'track': 'path', 'algorithm':'label'}, axis=1, inplace=True)\ndf = pd.concat((original_labels, unseen))\ndf.loc[df[\"label\"] != 5, ['label']] = 0\ndf","metadata":{"execution":{"iopub.status.busy":"2022-03-31T02:12:16.338286Z","iopub.execute_input":"2022-03-31T02:12:16.338531Z","iopub.status.idle":"2022-03-31T02:12:16.699596Z","shell.execute_reply.started":"2022-03-31T02:12:16.338499Z","shell.execute_reply":"2022-03-31T02:12:16.698787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Filter broken and non-existed paths\n\nprint(f\"Step 0: {len(df)}\")\n\ndf[\"status\"] = df[\"path\"].apply(lambda path: True if os.path.exists(path) else None)\ndf = df.dropna(subset=[\"path\"])\ndf = df.drop(\"status\", 1)\nprint(f\"Step 1: {len(df)}\")\n\ndf = df.sample(frac=1)\ndf = df.reset_index(drop=True)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-31T02:12:16.703827Z","iopub.execute_input":"2022-03-31T02:12:16.705955Z","iopub.status.idle":"2022-03-31T02:12:25.646027Z","shell.execute_reply.started":"2022-03-31T02:12:16.705908Z","shell.execute_reply":"2022-03-31T02:12:25.645308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Labels: \", df[\"label\"].unique())\nprint()\ndf.groupby(\"label\").count()[[\"path\"]]","metadata":{"execution":{"iopub.status.busy":"2022-03-31T02:12:25.64937Z","iopub.execute_input":"2022-03-31T02:12:25.650074Z","iopub.status.idle":"2022-03-31T02:12:25.664494Z","shell.execute_reply.started":"2022-03-31T02:12:25.650036Z","shell.execute_reply":"2022-03-31T02:12:25.663706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Showing a Random Audio Track","metadata":{}},{"cell_type":"code","source":"import torchaudio\nimport librosa\nimport IPython.display as ipd\nimport numpy as np\n\nidx = np.random.randint(0, len(df))\nsample = df.iloc[idx]\npath = sample[\"path\"]\nlabel = sample[\"label\"]\n\n\nprint(f\"ID Location: {idx}\")\nprint(f\"      Label: {label}\")\nprint()\n\nspeech, sr = torchaudio.load(path)\nspeech = speech[0].numpy().squeeze()\nspeech = librosa.resample(np.asarray(speech), sr, 16_000)\nipd.Audio(data=np.asarray(speech), autoplay=True, rate=16000)","metadata":{"execution":{"iopub.status.busy":"2022-03-31T02:12:25.66596Z","iopub.execute_input":"2022-03-31T02:12:25.666572Z","iopub.status.idle":"2022-03-31T02:12:26.834477Z","shell.execute_reply.started":"2022-03-31T02:12:25.666536Z","shell.execute_reply":"2022-03-31T02:12:26.833772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir data\nsave_path = \"./data\"\n\ntrain_df, test_df = train_test_split(df, test_size=0.1, random_state=85, stratify=df[\"label\"])\n\ntrain_df = train_df.reset_index(drop=True)\ntest_df = test_df.reset_index(drop=True)\n\ntrain_df.to_csv(f\"{save_path}/train.csv\", sep=\"\\t\", encoding=\"utf-8\", index=False)\ntest_df.to_csv(f\"{save_path}/test.csv\", sep=\"\\t\", encoding=\"utf-8\", index=False)\n\n\nprint(train_df.shape)\nprint(test_df.shape)","metadata":{"execution":{"iopub.status.busy":"2022-03-31T02:12:26.835435Z","iopub.execute_input":"2022-03-31T02:12:26.835681Z","iopub.status.idle":"2022-03-31T02:12:27.551668Z","shell.execute_reply.started":"2022-03-31T02:12:26.835642Z","shell.execute_reply":"2022-03-31T02:12:27.550828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loading the created dataset using datasets\nfrom datasets import load_dataset, load_metric\n\n\ndata_files = {\n    \"train\": \"./data/train.csv\", \n    \"validation\": \"./data/test.csv\",\n}\n\ndataset = load_dataset(\"csv\", data_files=data_files, delimiter=\"\\t\", )\ntrain_dataset = dataset[\"train\"]\neval_dataset = dataset[\"validation\"]\n\nprint(train_dataset)\nprint(eval_dataset)","metadata":{"execution":{"iopub.status.busy":"2022-03-31T02:12:27.553036Z","iopub.execute_input":"2022-03-31T02:12:27.553342Z","iopub.status.idle":"2022-03-31T02:12:28.57445Z","shell.execute_reply.started":"2022-03-31T02:12:27.553304Z","shell.execute_reply":"2022-03-31T02:12:28.573688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We need to specify the input and output column\ninput_column = \"path\"\noutput_column = \"label\"","metadata":{"execution":{"iopub.status.busy":"2022-03-31T02:12:28.575892Z","iopub.execute_input":"2022-03-31T02:12:28.576375Z","iopub.status.idle":"2022-03-31T02:12:28.579851Z","shell.execute_reply.started":"2022-03-31T02:12:28.576338Z","shell.execute_reply":"2022-03-31T02:12:28.579176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we need to distinguish the unique labels in our SER dataset\nlabel_list = train_dataset.unique(output_column)\nlabel_list.sort()  # Let's sort it for determinism\nnum_labels = len(label_list)\nprint(f\"A classification problem with {num_labels} classes: {label_list}\")","metadata":{"execution":{"iopub.status.busy":"2022-03-31T02:12:28.582008Z","iopub.execute_input":"2022-03-31T02:12:28.582486Z","iopub.status.idle":"2022-03-31T02:12:28.597551Z","shell.execute_reply.started":"2022-03-31T02:12:28.58245Z","shell.execute_reply":"2022-03-31T02:12:28.596736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoConfig, Wav2Vec2Processor","metadata":{"execution":{"iopub.status.busy":"2022-03-31T02:12:28.599168Z","iopub.execute_input":"2022-03-31T02:12:28.599627Z","iopub.status.idle":"2022-03-31T02:12:28.875675Z","shell.execute_reply.started":"2022-03-31T02:12:28.599593Z","shell.execute_reply":"2022-03-31T02:12:28.874929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name_or_path = \"lighteternal/wav2vec2-large-xlsr-53-greek\"\npooling_mode = \"mean\"","metadata":{"execution":{"iopub.status.busy":"2022-03-31T02:12:28.877034Z","iopub.execute_input":"2022-03-31T02:12:28.877301Z","iopub.status.idle":"2022-03-31T02:12:28.88308Z","shell.execute_reply.started":"2022-03-31T02:12:28.877267Z","shell.execute_reply":"2022-03-31T02:12:28.882276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# config\nconfig = AutoConfig.from_pretrained(\n    model_name_or_path,\n    num_labels=num_labels,\n    label2id={label: i for i, label in enumerate(label_list)},\n    id2label={i: label for i, label in enumerate(label_list)},\n    finetuning_task=\"wav2vec2_clf\",\n)\nsetattr(config, 'pooling_mode', pooling_mode)","metadata":{"execution":{"iopub.status.busy":"2022-03-31T02:12:28.884379Z","iopub.execute_input":"2022-03-31T02:12:28.885206Z","iopub.status.idle":"2022-03-31T02:12:29.630104Z","shell.execute_reply.started":"2022-03-31T02:12:28.885164Z","shell.execute_reply":"2022-03-31T02:12:29.629317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"processor = Wav2Vec2Processor.from_pretrained(model_name_or_path,)\ntarget_sampling_rate = processor.feature_extractor.sampling_rate\nprint(f\"The target sampling rate: {target_sampling_rate}\")","metadata":{"execution":{"iopub.status.busy":"2022-03-31T02:12:29.631422Z","iopub.execute_input":"2022-03-31T02:12:29.631744Z","iopub.status.idle":"2022-03-31T02:12:33.851361Z","shell.execute_reply.started":"2022-03-31T02:12:29.631705Z","shell.execute_reply":"2022-03-31T02:12:33.850636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def speech_file_to_array_fn(path):\n    speech_array, sampling_rate = torchaudio.load(path)\n    resampler = torchaudio.transforms.Resample(sampling_rate, target_sampling_rate)\n    speech = resampler(speech_array).squeeze().numpy()\n    return speech\n\ndef label_to_id(label, label_list):\n\n    if len(label_list) > 0:\n        return label_list.index(label) if label in label_list else -1\n\n    return label\n\ndef preprocess_function(examples):\n    speech_list = [speech_file_to_array_fn(path) for path in examples[input_column]]\n    target_list = [label_to_id(label, label_list) for label in examples[output_column]]\n\n    result = processor(speech_list, sampling_rate=target_sampling_rate)\n    result[\"labels\"] = list(target_list)\n\n    return result","metadata":{"execution":{"iopub.status.busy":"2022-03-31T02:12:33.85295Z","iopub.execute_input":"2022-03-31T02:12:33.853488Z","iopub.status.idle":"2022-03-31T02:12:33.861832Z","shell.execute_reply.started":"2022-03-31T02:12:33.853449Z","shell.execute_reply":"2022-03-31T02:12:33.860915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = train_dataset.map(\n    preprocess_function,\n    batch_size=100,\n    batched=True,\n    num_proc=4\n)\neval_dataset = eval_dataset.map(\n    preprocess_function,\n    batch_size=100,\n    batched=True,\n    num_proc=4\n)","metadata":{"execution":{"iopub.status.busy":"2022-03-31T02:12:33.864431Z","iopub.execute_input":"2022-03-31T02:12:33.865172Z","iopub.status.idle":"2022-03-31T02:13:19.703189Z","shell.execute_reply.started":"2022-03-31T02:12:33.865143Z","shell.execute_reply":"2022-03-31T02:13:19.702331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"idx = 0\nprint(f\"Training input_values: {train_dataset[idx]['input_values']}\")\nprint(f\"Training attention_mask: {train_dataset[idx]['attention_mask']}\")\nprint(f\"Training labels: {train_dataset[idx]['labels']} - {train_dataset[idx]['label']}\")","metadata":{"execution":{"iopub.status.busy":"2022-03-31T02:13:19.704949Z","iopub.execute_input":"2022-03-31T02:13:19.705227Z","iopub.status.idle":"2022-03-31T02:13:20.136536Z","shell.execute_reply.started":"2022-03-31T02:13:19.705196Z","shell.execute_reply":"2022-03-31T02:13:20.135706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from dataclasses import dataclass\nfrom typing import Optional, Tuple\nimport torch\nfrom transformers.file_utils import ModelOutput\n\n\n@dataclass\nclass SpeechClassifierOutput(ModelOutput):\n    loss: Optional[torch.FloatTensor] = None\n    logits: torch.FloatTensor = None\n    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n    attentions: Optional[Tuple[torch.FloatTensor]] = None\n","metadata":{"execution":{"iopub.status.busy":"2022-03-31T02:13:20.138064Z","iopub.execute_input":"2022-03-31T02:13:20.138315Z","iopub.status.idle":"2022-03-31T02:13:21.875354Z","shell.execute_reply.started":"2022-03-31T02:13:20.138284Z","shell.execute_reply":"2022-03-31T02:13:21.874354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Classifier","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n\nfrom transformers.models.wav2vec2.modeling_wav2vec2 import (\n    Wav2Vec2PreTrainedModel,\n    Wav2Vec2Model\n)\n\n\nclass Wav2Vec2ClassificationHead(nn.Module):\n    \"\"\"Head for wav2vec classification task.\"\"\"\n\n    def __init__(self, config):\n        super().__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.dropout = nn.Dropout(config.final_dropout)\n        self.out_proj = nn.Linear(config.hidden_size, config.num_labels)\n\n    def forward(self, features, **kwargs):\n        x = features\n        x = self.dropout(x)\n        x = self.dense(x)\n        x = torch.tanh(x)\n        x = self.dropout(x)\n        x = self.out_proj(x)\n        return x\n\n\nclass Wav2Vec2ForSpeechClassification(Wav2Vec2PreTrainedModel):\n    def __init__(self, config):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n        self.pooling_mode = config.pooling_mode\n        self.config = config\n\n        self.wav2vec2 = Wav2Vec2Model(config)\n        self.classifier = Wav2Vec2ClassificationHead(config)\n\n        self.init_weights()\n\n    def freeze_feature_extractor(self):\n        self.wav2vec2.feature_extractor._freeze_parameters()\n\n    def merged_strategy(\n            self,\n            hidden_states,\n            mode=\"mean\"\n    ):\n        if mode == \"mean\":\n            outputs = torch.mean(hidden_states, dim=1)\n        elif mode == \"sum\":\n            outputs = torch.sum(hidden_states, dim=1)\n        elif mode == \"max\":\n            outputs = torch.max(hidden_states, dim=1)[0]\n        else:\n            raise Exception(\n                \"The pooling method hasn't been defined! Your pooling mode must be one of these ['mean', 'sum', 'max']\")\n\n        return outputs\n\n    def forward(\n            self,\n            input_values,\n            attention_mask=None,\n            output_attentions=None,\n            output_hidden_states=None,\n            return_dict=None,\n            labels=None,\n    ):\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n        outputs = self.wav2vec2(\n            input_values,\n            attention_mask=attention_mask,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        hidden_states = outputs[0]\n        hidden_states = self.merged_strategy(hidden_states, mode=self.pooling_mode)\n        logits = self.classifier(hidden_states)\n\n        loss = None\n        if labels is not None:\n            if self.config.problem_type is None:\n                if self.num_labels == 1:\n                    self.config.problem_type = \"regression\"\n                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n                    self.config.problem_type = \"single_label_classification\"\n                else:\n                    self.config.problem_type = \"multi_label_classification\"\n\n            if self.config.problem_type == \"regression\":\n                loss_fct = MSELoss()\n                loss = loss_fct(logits.view(-1, self.num_labels), labels)\n            elif self.config.problem_type == \"single_label_classification\":\n                loss_fct = CrossEntropyLoss()\n                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n            elif self.config.problem_type == \"multi_label_classification\":\n                loss_fct = BCEWithLogitsLoss()\n                loss = loss_fct(logits, labels)\n\n        if not return_dict:\n            output = (logits,) + outputs[2:]\n            return ((loss,) + output) if loss is not None else output\n\n        return SpeechClassifierOutput(\n            loss=loss,\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n","metadata":{"execution":{"iopub.status.busy":"2022-03-31T02:13:21.878621Z","iopub.execute_input":"2022-03-31T02:13:21.878931Z","iopub.status.idle":"2022-03-31T02:13:21.943168Z","shell.execute_reply.started":"2022-03-31T02:13:21.878891Z","shell.execute_reply":"2022-03-31T02:13:21.942435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from dataclasses import dataclass\nfrom typing import Dict, List, Optional, Union\nimport torch\n\nimport transformers\nfrom transformers import Wav2Vec2Processor\n\n\n@dataclass\nclass DataCollatorCTCWithPadding:\n    \"\"\"\n    Data collator that will dynamically pad the inputs received.\n    Args:\n        processor (:class:`~transformers.Wav2Vec2Processor`)\n            The processor used for proccessing the data.\n        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n            among:\n            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n              sequence if provided).\n            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n              maximum acceptable input length for the model if that argument is not provided.\n            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n              different lengths).\n        max_length (:obj:`int`, `optional`):\n            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).\n        max_length_labels (:obj:`int`, `optional`):\n            Maximum length of the ``labels`` returned list and optionally padding length (see above).\n        pad_to_multiple_of (:obj:`int`, `optional`):\n            If set will pad the sequence to a multiple of the provided value.\n            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n            7.5 (Volta).\n    \"\"\"\n\n    processor: Wav2Vec2Processor\n    padding: Union[bool, str] = True\n    max_length: Optional[int] = None\n    max_length_labels: Optional[int] = None\n    pad_to_multiple_of: Optional[int] = None\n    pad_to_multiple_of_labels: Optional[int] = None\n\n    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n        label_features = [feature[\"labels\"] for feature in features]\n\n        d_type = torch.long if isinstance(label_features[0], int) else torch.float\n\n        batch = self.processor.pad(\n            input_features,\n            padding=self.padding,\n            max_length=self.max_length,\n            pad_to_multiple_of=self.pad_to_multiple_of,\n            return_tensors=\"pt\",\n        )\n\n        batch[\"labels\"] = torch.tensor(label_features, dtype=d_type)\n\n        return batch","metadata":{"execution":{"iopub.status.busy":"2022-03-31T02:13:21.944595Z","iopub.execute_input":"2022-03-31T02:13:21.944849Z","iopub.status.idle":"2022-03-31T02:13:21.955539Z","shell.execute_reply.started":"2022-03-31T02:13:21.944814Z","shell.execute_reply":"2022-03-31T02:13:21.954869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-31T02:13:21.956922Z","iopub.execute_input":"2022-03-31T02:13:21.957418Z","iopub.status.idle":"2022-03-31T02:13:21.967642Z","shell.execute_reply.started":"2022-03-31T02:13:21.957365Z","shell.execute_reply":"2022-03-31T02:13:21.966848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"is_regression = False","metadata":{"execution":{"iopub.status.busy":"2022-03-31T02:13:21.970134Z","iopub.execute_input":"2022-03-31T02:13:21.970562Z","iopub.status.idle":"2022-03-31T02:13:21.97706Z","shell.execute_reply.started":"2022-03-31T02:13:21.970534Z","shell.execute_reply":"2022-03-31T02:13:21.976272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom transformers import EvalPrediction\n\n\ndef compute_metrics(p: EvalPrediction):\n    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n    preds = np.squeeze(preds) if is_regression else np.argmax(preds, axis=1)\n\n    if is_regression:\n        return {\"mse\": ((preds - p.label_ids) ** 2).mean().item()}\n    else:\n        return {\"accuracy\": (preds == p.label_ids).astype(np.float32).mean().item()}","metadata":{"execution":{"iopub.status.busy":"2022-03-31T02:13:21.981813Z","iopub.execute_input":"2022-03-31T02:13:21.982417Z","iopub.status.idle":"2022-03-31T02:13:23.506878Z","shell.execute_reply.started":"2022-03-31T02:13:21.982383Z","shell.execute_reply":"2022-03-31T02:13:23.506152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Wav2Vec2ForSpeechClassification.from_pretrained(\n    model_name_or_path,\n    config=config,\n)","metadata":{"execution":{"iopub.status.busy":"2022-03-31T02:13:23.508229Z","iopub.execute_input":"2022-03-31T02:13:23.508471Z","iopub.status.idle":"2022-03-31T02:14:03.762442Z","shell.execute_reply.started":"2022-03-31T02:13:23.508439Z","shell.execute_reply":"2022-03-31T02:14:03.761767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.freeze_feature_extractor()","metadata":{"execution":{"iopub.status.busy":"2022-03-31T02:14:03.763658Z","iopub.execute_input":"2022-03-31T02:14:03.763993Z","iopub.status.idle":"2022-03-31T02:14:03.768187Z","shell.execute_reply.started":"2022-03-31T02:14:03.763955Z","shell.execute_reply":"2022-03-31T02:14:03.767434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir=\"./wav2vec2-xlsr-greek-speech-emotion-recognition\",\n    # output_dir=\"/content/gdrive/MyDrive/wav2vec2-xlsr-greek-speech-emotion-recognition\"\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    gradient_accumulation_steps=2,\n    evaluation_strategy=\"steps\",\n    num_train_epochs=1.0,\n    fp16=True,\n    save_steps=10,\n    eval_steps=10,\n    logging_steps=10,\n    learning_rate=1e-4,\n    save_total_limit=1,\n)","metadata":{"execution":{"iopub.status.busy":"2022-03-31T02:14:03.769746Z","iopub.execute_input":"2022-03-31T02:14:03.77038Z","iopub.status.idle":"2022-03-31T02:14:03.914087Z","shell.execute_reply.started":"2022-03-31T02:14:03.770344Z","shell.execute_reply":"2022-03-31T02:14:03.913329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from typing import Any, Dict, Union\n\nimport torch\nfrom packaging import version\nfrom torch import nn\n\nfrom transformers import (\n    Trainer,\n    is_apex_available,\n)\n\nif is_apex_available():\n    from apex import amp\n\nif version.parse(torch.__version__) >= version.parse(\"1.6\"):\n    _is_native_amp_available = True\n    from torch.cuda.amp import autocast\n\n\nclass CTCTrainer(Trainer):\n    def training_step(self, model: nn.Module, inputs: Dict[str, Union[torch.Tensor, Any]]) -> torch.Tensor:\n        \"\"\"\n        Perform a training step on a batch of inputs.\n\n        Subclass and override to inject custom behavior.\n\n        Args:\n            model (:obj:`nn.Module`):\n                The model to train.\n            inputs (:obj:`Dict[str, Union[torch.Tensor, Any]]`):\n                The inputs and targets of the model.\n\n                The dictionary will be unpacked before being fed to the model. Most models expect the targets under the\n                argument :obj:`labels`. Check your model's documentation for all accepted arguments.\n\n        Return:\n            :obj:`torch.Tensor`: The tensor with training loss on this batch.\n        \"\"\"\n\n        model.train()\n        inputs = self._prepare_inputs(inputs)\n\n        if self.use_amp:\n            with autocast():\n                loss = self.compute_loss(model, inputs)\n        else:\n            loss = self.compute_loss(model, inputs)\n\n        if self.args.gradient_accumulation_steps > 1:\n            loss = loss / self.args.gradient_accumulation_steps\n\n        if self.use_amp:\n            self.scaler.scale(loss).backward()\n        elif self.use_apex:\n            with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n                scaled_loss.backward()\n        elif self.deepspeed:\n            self.deepspeed.backward(loss)\n        else:\n            loss.backward()\n\n        return loss.detach()\n","metadata":{"execution":{"iopub.status.busy":"2022-03-31T02:14:03.915537Z","iopub.execute_input":"2022-03-31T02:14:03.915776Z","iopub.status.idle":"2022-03-31T02:14:04.105658Z","shell.execute_reply.started":"2022-03-31T02:14:03.915742Z","shell.execute_reply":"2022-03-31T02:14:04.104965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer = CTCTrainer(\n    model=model,\n    data_collator=data_collator,\n    args=training_args,\n    compute_metrics=compute_metrics,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    tokenizer=processor.feature_extractor,\n)","metadata":{"execution":{"iopub.status.busy":"2022-03-31T02:14:04.107007Z","iopub.execute_input":"2022-03-31T02:14:04.107283Z","iopub.status.idle":"2022-03-31T02:14:09.563746Z","shell.execute_reply.started":"2022-03-31T02:14:04.107247Z","shell.execute_reply":"2022-03-31T02:14:09.562949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2022-03-31T02:14:09.565351Z","iopub.execute_input":"2022-03-31T02:14:09.565562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !mkdir \"./data\"\n\nimport pandas as pd\n\ntest = pd.read_csv('../input/spcup2022/spcup_2022_eval_part1/spcup_2022_eval_part1/labels_eval_part1.csv')\ntest = '../input/spcup2022/spcup_2022_eval_part1/spcup_2022_eval_part1' + '/' + test.iloc[:, 1]\n\nsave_path = \"./data\"\n\ntest = test.reset_index(drop=True)\n\ntest.to_csv(f\"{save_path}/test.csv\", sep=\"\\t\", encoding=\"utf-8\", index=False)\n\nprint(test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prediction and Evalution","metadata":{}},{"cell_type":"markdown","source":"### Eval Part 1","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchaudio\ntarget_sampling_rate = 16000\n\ndef speech_file_to_array_fn(path, sampling_rate):\n    speech_array, _sampling_rate = torchaudio.load(path)\n    resampler = torchaudio.transforms.Resample(_sampling_rate)\n    speech = resampler(speech_array).squeeze().numpy()\n    return speech\n\n\ndef predict(path, sampling_rate):\n    device = \"cuda\"\n    speech = speech_file_to_array_fn(path, sampling_rate)\n    features = processor(speech, sampling_rate=sampling_rate, return_tensors=\"pt\", padding=True)\n\n    input_values = features.input_values.to(device)\n    attention_mask = features.attention_mask.to(device)\n\n    with torch.no_grad():\n        logits = model(input_values, attention_mask=attention_mask).logits\n\n    scores = F.softmax(logits, dim=1).detach().cpu().numpy()[0]\n    outputs = [{\"label\": config.id2label[i], \"Score\": f\"{round(score * 100, 3):.1f}%\"} for i, score in enumerate(scores)]\n    return outputs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def find_max(ls):\n    idx_maximum = 0\n    maximum_score = np.float16(temp[idx_maximum]['Score'][0:3]) \n    for i in range(len(ls)):\n        temp1 = np.float16(temp[i]['Score'][0:3])\n        if maximum_score < temp1:\n            idx_maximum = i\n            maximum_score = np.float16(temp[idx_maximum]['Score'][0:3]) \n    return idx_maximum","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path1 = \"../input/spcup2022/spcup_2022_eval_part1/spcup_2022_eval_part1\"\nfor files in os.listdir(\"../input/spcup2022/spcup_2022_eval_part1/spcup_2022_eval_part1\"):\n    if files.endswith(\".csv\"):\n        eval1 = pd.read_csv(path1 + \"/\" + files)\neval1 = path1 + \"/\" + eval1.iloc[:, 1]\neval1 = pd.DataFrame(eval1)\neval1['label'] = 16000\neval1.rename(columns = {\"track\":\"path\"}, inplace = True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range (len(eval1)):\n    temp = predict(eval1.iloc[i,0], 16000)\n    eval1.iloc[i, 1] = find_max(temp)\neval1.to_csv(\"./EVAL1.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Eval Part 2","metadata":{}},{"cell_type":"code","source":"!wget https://www.dropbox.com/s/zylz07o2z0x308g/spcup_2022_eval_part2.zip?dl=1\n!mv spcup_2022_eval_part2.zip?dl=1 spcup_2022_eval_part2.zip\n!unzip spcup_2022_eval_part2.zip\n!rm spcup_2022_eval_part2.zip","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path2 = \"./spcup_2022_eval_part2\"\nfor files in os.listdir(\"./spcup_2022_eval_part2\"):\n    if files.endswith(\".csv\"):\n        eval2 = pd.read_csv(path2 + \"/\" + files)\neval2 = path2 + \"/\" + eval2.iloc[:, 1]\neval2 = pd.DataFrame(eval2)\neval2['label'] = 16000\neval2.rename(columns = {\"track\":\"path\"}, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2022-03-31T15:14:43.721817Z","iopub.execute_input":"2022-03-31T15:14:43.722384Z","iopub.status.idle":"2022-03-31T15:14:43.802355Z","shell.execute_reply.started":"2022-03-31T15:14:43.722294Z","shell.execute_reply":"2022-03-31T15:14:43.801533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range (len(eval2)):\n    temp = predict(eval2.iloc[i,0], 16000)\n    eval2.iloc[i, 1] = find_max(temp)\n    print(\"Percentage has been compeleted: \" + str(i/len(eval2) * 100))\neval2.to_csv(\"./EVAL2.csv\")","metadata":{},"execution_count":null,"outputs":[]}]}